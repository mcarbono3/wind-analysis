import os
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
import joblib

# === Ruta robusta al CSV ===
base_dir = os.path.dirname(__file__)
data_path = os.path.join(base_dir, 'database', 'parsed_hurdat_data.csv')

# === Cargar los datos parseados ===
df = pd.read_csv(data_path, dtype={"date": str, "time": str})

# Define Colombian Caribbean bounding box (approximate)
caribbean_bbox = {
    "min_lat": 10.0,
    "max_lat": 13.0,
    "min_lon": -82.0,
    "max_lon": -74.0
}

# For demonstration, let's assume we have multiple analysis points and their corresponding metrics.
# In a real scenario, these would be generated by iterating through various geographical points.
# For now, we'll simulate this by creating a dummy dataset based on the existing metrics.

# Dummy data generation for demonstration purposes
# In a real application, you would run the statistical_geospatial_analysis.py for many points
# and collect the results into a DataFrame.

# Example metrics from the previous run (for a single point)
# event_density: 15
# event_frequency: {'DB': 0.06666666666666667, 'HU': 0.2, 'TD': 0.4, 'TS': 0.4}
# event_intensity_profile: {'DB': 35.0, 'HU': 72.85714285714286, 'TD': 27.0, 'TS': 45.55555555555556}
# energy_opportunity_score: 0.7333333333333333
# extreme_risk_index: 0.2
# event_duration_stats: {'average_duration_hours': np.float64(4.8), 'std_dev_duration_hours': np.float64(6.4939532313859925)}
# historical_pressure_min: 980

# Let's create a synthetic dataset for training
data = {
    'event_density': [15, 10, 20, 5, 25, 12, 18, 8, 22, 14],
    'avg_wind_speed_hu': [72.8, 60.0, 80.0, 50.0, 90.0, 65.0, 75.0, 55.0, 85.0, 70.0],
    'avg_wind_speed_ts': [45.5, 40.0, 50.0, 35.0, 55.0, 42.0, 48.0, 38.0, 52.0, 44.0],
    'energy_opportunity_score': [0.73, 0.6, 0.8, 0.5, 0.9, 0.65, 0.78, 0.55, 0.85, 0.70],
    'extreme_risk_index': [0.2, 0.1, 0.3, 0.05, 0.4, 0.15, 0.25, 0.08, 0.35, 0.18],
    'avg_duration_hours': [4.8, 3.0, 6.0, 2.0, 7.0, 3.5, 5.5, 2.5, 6.5, 4.0],
    'min_pressure': [980, 990, 970, 1000, 960, 985, 975, 995, 965, 982]
}
df_ml = pd.DataFrame(data)

# Define 'impacto_neto' based on a simple rule for demonstration
# This is a placeholder and should be replaced with actual domain knowledge or labeled data
def get_impacto_neto(row):
    if row['energy_opportunity_score'] > 0.7 and row['extreme_risk_index'] < 0.2:
        return 'positivo'
    elif row['energy_opportunity_score'] < 0.6 and row['extreme_risk_index'] > 0.3:
        return 'negativo'
    else:
        return 'neutral'

df_ml['impacto_neto'] = df_ml.apply(get_impacto_neto, axis=1)

# Prepare data for training
X = df_ml.drop('impacto_neto', axis=1)
y = df_ml['impacto_neto']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train a RandomForestClassifier model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Make predictions and evaluate the model
y_pred = model.predict(X_test)
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Save the trained model
model_filename = 'random_forest_model.joblib'
joblib.dump(model, model_filename)
print(f"\nModelo entrenado guardado como {model_filename}")

